# üöÄ Guide d'ex√©cution unique - Tokyo Cheapo Crawler

## 1Ô∏è‚É£ Pr√©paration (5 minutes)

### A. Configuration des cl√©s API
```bash
# Cr√©ez votre fichier .env
cp .env.example .env

# √âditez .env avec vos vraies cl√©s :
# - SUPABASE_URL
# - SUPABASE_SERVICE_KEY  
# - OPENAI_API_KEY
# - SCRAPINGBEE_API_KEY
```

### B. Installation Python
```bash
# Installer les d√©pendances
pip install -r requirements.txt
```

### C. Setup base de donn√©es Supabase
1. Allez dans l'√©diteur SQL de Supabase
2. Copiez-collez le contenu de `database_setup.sql`
3. Ex√©cutez le script

## 2Ô∏è‚É£ Lancement du crawler

```bash
# C'est parti ! 
python main_crawler.py
```

## 3Ô∏è‚É£ Que va-t-il se passer ?

1. **D√©couverte** : Le script va r√©cup√©rer les ~52 articles de Tokyo Cheapo
2. **Traitement** : Pour chaque article :
   - T√©l√©chargement du contenu (ScrapingBee)
   - Classification POI/Non-POI (GPT-3.5)
   - Reformulation unique (GPT-4)
   - Extraction des donn√©es (nom, quartier, etc.)
   - Sauvegarde en base comme brouillon
3. **Dur√©e estim√©e** : ~30-45 minutes pour tout le site
4. **Co√ªt estim√©** : ~5-10$ en tokens OpenAI

## 4Ô∏è‚É£ Suivi en temps r√©el

### Dans le terminal
```
üöÄ D√©marrage du crawler intelligent Yorimichi
üì• T√©l√©chargement du sitemap: https://tokyocheapo.com/post-sitemap.xml
‚úÖ 52 URLs trouv√©es dans le sitemap
üìã 52 nouvelles URLs √† traiter
üîÑ Traitement de: https://tokyocheapo.com/entertainment/nakano-broadway/
‚úÖ POI cr√©√©: Nakano Broadway
...
```

### Dans Supabase (pendant l'ex√©cution)
```sql
-- Voir la progression
SELECT status, COUNT(*) 
FROM processed_urls 
GROUP BY status;

-- Voir les POIs cr√©√©s
SELECT name, substring(description, 1, 100) as preview 
FROM locations 
WHERE is_active = false 
ORDER BY created_at DESC;
```

## 5Ô∏è‚É£ Apr√®s l'ex√©cution

### R√©sultats attendus
- ~20-30 POIs cr√©√©s (beaucoup d'articles ne sont pas des lieux)
- Tous marqu√©s comme `is_active = false` (brouillons)
- Logs complets dans `yorimichi_crawler.log`

### V√©rifier les r√©sultats
```sql
-- Stats finales
SELECT 
  (SELECT COUNT(*) FROM locations WHERE source_name = 'Tokyo Cheapo') as pois_created,
  (SELECT COUNT(*) FROM processed_urls WHERE status = 'success') as successful,
  (SELECT COUNT(*) FROM processed_urls WHERE status = 'skipped_not_a_poi') as not_pois,
  (SELECT COUNT(*) FROM processed_urls WHERE status = 'skipped_duplicate') as duplicates;
```

## 6Ô∏è‚É£ Troubleshooting

### Si √ßa s'arr√™te en cours de route
Pas de panique ! Relancez simplement :
```bash
python main_crawler.py
```
Le script reprendra automatiquement o√π il s'est arr√™t√©.

### Erreurs courantes
- **"Rate limit exceeded"** : Attendez 1 minute et relancez
- **"Timeout"** : Normal pour certaines pages, le script continue
- **"Not a POI"** : Normal, beaucoup d'articles sont des guides g√©n√©raux

## 7Ô∏è‚É£ Nettoyage (optionnel)

Si vous voulez recommencer √† z√©ro :
```sql
-- ‚ö†Ô∏è ATTENTION : Supprime TOUT
DELETE FROM locations WHERE source_name = 'Tokyo Cheapo';
DELETE FROM processed_urls;
DELETE FROM agent_logs WHERE agent_name = 'Intelligent Crawler V2';
```

---

**Temps total estim√© : 30-45 minutes**
**Co√ªt total estim√© : 5-10$ (tokens OpenAI)**

Bonne collecte ! üéâ