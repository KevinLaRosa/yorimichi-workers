# Yorimichi Intelligent Crawler V2

Un agent de collecte intelligent qui parcourt des sites web pour identifier et crÃ©er automatiquement des Points d'IntÃ©rÃªt (POI) uniques dans votre base de donnÃ©es Supabase.

## ğŸ¯ FonctionnalitÃ©s Principales

- **Crawling Intelligent** : Parcourt automatiquement les sitemaps et identifie les articles pertinents
- **Classification IA** : Utilise GPT-3.5 pour dÃ©terminer si un article dÃ©crit un POI
- **Reformulation Unique** : GÃ©nÃ¨re des descriptions originales avec GPT-4 pour Ã©viter le plagiat
- **Extraction StructurÃ©e** : Extrait automatiquement nom, quartier, rÃ©sumÃ© et mots-clÃ©s
- **DÃ©tection de Doublons** : Utilise des embeddings pour Ã©viter les doublons sÃ©mantiques
- **Reprise sur Erreur** : SystÃ¨me robuste et idempotent qui peut reprendre oÃ¹ il s'est arrÃªtÃ©
- **Monitoring Complet** : Logs dÃ©taillÃ©s en base de donnÃ©es pour suivre l'activitÃ©

## ğŸš€ Installation

### 1. PrÃ©requis

- Python 3.8+
- Un projet Supabase configurÃ©
- Des clÃ©s API pour OpenAI et ScrapingBee

### 2. Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. Configuration de la base de donnÃ©es

ExÃ©cutez le script SQL fourni dans votre projet Supabase :

```bash
# Dans l'Ã©diteur SQL de Supabase, copiez-collez le contenu de database_setup.sql
```

### 4. Configuration des variables d'environnement

```bash
cp .env.example .env
# Ã‰ditez .env avec vos vraies clÃ©s API
```

## ğŸ“‹ Configuration

Le crawler est configurÃ© par dÃ©faut pour :
- Site cible : Tokyo Cheapo
- ModÃ¨les IA : GPT-3.5 pour la classification, GPT-4 pour la reformulation
- DÃ©lai entre requÃªtes : 1 seconde
- Logs de progression : toutes les 25 URLs

Vous pouvez modifier ces paramÃ¨tres dans la classe `YorimichiIntelligentCrawler`.

## ğŸƒ Utilisation

### Lancement simple

```bash
python main_crawler.py
```

### Monitoring en temps rÃ©el

Pendant l'exÃ©cution, le crawler :
- Affiche les logs dans la console
- Enregistre les logs dans `yorimichi_crawler.log`
- Sauvegarde l'Ã©tat dans la base de donnÃ©es

### Suivi dans Supabase

```sql
-- Voir la progression globale
SELECT status, COUNT(*) 
FROM processed_urls 
GROUP BY status;

-- Voir les logs rÃ©cents
SELECT * FROM agent_logs 
ORDER BY created_at DESC 
LIMIT 20;

-- Voir les POIs crÃ©Ã©s (brouillons)
SELECT * FROM locations 
WHERE is_active = false 
ORDER BY created_at DESC;
```

## ğŸ”„ Reprise sur Erreur

Le crawler est conÃ§u pour Ãªtre relancÃ© sans risque :
- Il reprend automatiquement oÃ¹ il s'est arrÃªtÃ©
- Les URLs dÃ©jÃ  traitÃ©es ne sont pas re-tÃ©lÃ©chargÃ©es
- Les erreurs n'arrÃªtent pas le processus global

## ğŸ“Š Structure des DonnÃ©es

### Table `locations` (POI crÃ©Ã©s)
- `name` : Nom du lieu extrait
- `description` : Description unique reformulÃ©e
- `is_active` : false (brouillons par dÃ©faut)
- `source_url` : URL originale
- `source_name` : "Tokyo Cheapo"
- `features` : DonnÃ©es additionnelles (quartier, mots-clÃ©s)

### Table `processed_urls`
- `url` : URL traitÃ©e
- `status` : success, failed, skipped_not_a_poi, skipped_duplicate
- `processed_at` : Timestamp du traitement

### Table `agent_logs`
- Logs dÃ©taillÃ©s de l'activitÃ© du crawler
- Statuts : STARTED, RUNNING, SUCCESS, ERROR
- DÃ©tails JSON pour le debugging

## ğŸ›¡ï¸ Gestion des Erreurs

Le crawler gÃ¨re gracieusement :
- Timeouts rÃ©seau
- Erreurs API (OpenAI, ScrapingBee)
- Pages mal formÃ©es
- RÃ©ponses IA inattendues

Chaque erreur est loggÃ©e sans arrÃªter le processus global.

## ğŸ’° Optimisation des CoÃ»ts

- Utilise GPT-3.5 (moins cher) pour la classification initiale
- Limite la taille du texte envoyÃ© aux APIs
- Cache les embeddings dans la base
- Ã‰vite de retraiter les URLs dÃ©jÃ  vues

## ğŸ”§ Personnalisation

Pour adapter le crawler Ã  d'autres sites :

1. Modifiez `sitemap_url` dans `__init__`
2. Ajustez les sÃ©lecteurs BeautifulSoup dans `extract_text_content`
3. Personnalisez les prompts GPT selon vos besoins
4. Adaptez la structure de donnÃ©es extraites

## ğŸ“ˆ Performance

Sur un site typique :
- ~30-60 secondes par URL (incluant tous les appels API)
- ~100-200 POIs/heure selon la complexitÃ©
- CoÃ»t estimÃ© : ~0.10-0.20$ par POI crÃ©Ã©

## ğŸ› Debugging

En cas de problÃ¨me :

1. VÃ©rifiez les logs console et fichier
2. Consultez la table `agent_logs` dans Supabase
3. VÃ©rifiez vos quotas API (OpenAI, ScrapingBee)
4. Testez avec une seule URL en mode debug

## ğŸ“ License

Ce projet fait partie de Yorimichi et suit les mÃªmes conditions de licence.