#!/usr/bin/env python3
"""
Worker Intelligent Yorimichi V3 - SMART EDITION
Version optimis√©e qui combine qualit√© premium et √©conomie intelligente
"""

import os
import sys
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import traceback

# Imports externes
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from supabase import create_client, Client
import openai
from openai import OpenAI

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('yorimichi_crawler_smart.log')
    ]
)
logger = logging.getLogger('YorimichiSmartCrawler')


class YorimichiSmartCrawler:
    """Agent intelligent avec strat√©gie optimis√©e qualit√©/co√ªt"""
    
    # Sitemaps prioritaires (les meilleurs POIs d'abord)
    PRIORITY_SITEMAPS = {
        'high': [
            "https://tokyocheapo.com/place-sitemap1.xml",  # Attractions principales
            "https://tokyocheapo.com/place-sitemap2.xml",
            "https://tokyocheapo.com/place-sitemap3.xml",
        ],
        'medium': [
            "https://tokyocheapo.com/restaurant-sitemap1.xml",  # Restos populaires
            "https://tokyocheapo.com/accommodation-sitemap.xml",  # Hotels
        ],
        'low': [
            "https://tokyocheapo.com/restaurant-sitemap2.xml",  # Autres restos
            "https://tokyocheapo.com/event-sitemap1.xml",  # Events (temporaires)
            "https://tokyocheapo.com/event-sitemap2.xml",
            "https://tokyocheapo.com/tour-sitemap.xml",
        ]
    }
    
    def __init__(self, quality_mode='smart'):
        """
        Modes disponibles:
        - 'smart': GPT-3.5 pour classification, GPT-4 pour les POIs confirm√©s
        - 'premium': GPT-4 pour tout (plus cher)
        - 'economy': GPT-3.5 pour tout (moins cher)
        """
        # Charger les variables d'environnement
        if os.path.exists('.env.local'):
            load_dotenv('.env.local')
        else:
            load_dotenv()
        
        # Validation des variables d'environnement
        self.validate_environment()
        
        # Initialisation des clients
        self.supabase = create_client(
            os.getenv('NEXT_PUBLIC_SUPABASE_URL'),
            os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        )
        
        # Configuration OpenAI
        openai.api_key = os.getenv('OPENAI_API_KEY')
        self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # Configuration ScrapingBee
        self.scrapingbee_api_key = os.getenv('SCRAPINGBEE_API_KEY')
        
        # Mode de qualit√©
        self.quality_mode = quality_mode
        
        # Configuration des mod√®les selon le mode
        if quality_mode == 'premium':
            self.classification_model = "gpt-4-turbo-preview"
            self.generation_model = "gpt-4-turbo-preview"
            self.cost_per_url = 0.10
        elif quality_mode == 'economy':
            self.classification_model = "gpt-3.5-turbo"
            self.generation_model = "gpt-3.5-turbo"
            self.cost_per_url = 0.02
        else:  # smart (par d√©faut)
            self.classification_model = "gpt-3.5-turbo"
            self.generation_model = "gpt-4-turbo-preview"
            self.cost_per_url = 0.05
        
        # Param√®tres
        self.site_name = "Tokyo Cheapo"
        self.agent_name = f"Smart Crawler V3 ({quality_mode} mode)"
        self.batch_log_interval = 25
        self.delay_between_urls = 1.5
        
        # Compteurs
        self.processed_count = 0
        self.success_count = 0
        self.skip_count = 0
        self.error_count = 0
        self.total_cost_estimate = 0.0
        
    def validate_environment(self):
        """Valide que toutes les variables d'environnement requises sont pr√©sentes"""
        required_vars = [
            'NEXT_PUBLIC_SUPABASE_URL',
            'SUPABASE_SERVICE_ROLE_KEY',
            'OPENAI_API_KEY',
            'SCRAPINGBEE_API_KEY'
        ]
        
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            error_msg = f"Variables d'environnement manquantes: {', '.join(missing_vars)}"
            logger.error(error_msg)
            raise EnvironmentError(error_msg)
            
        logger.info("‚úÖ Toutes les variables d'environnement sont configur√©es")
        
    def verify_database_schema(self) -> Dict[str, bool]:
        """V√©rifie que le sch√©ma de base de donn√©es est compatible"""
        verification_results = {
            'locations_table': False,
            'processed_urls_table': False,
            'agent_logs_table': False,
            'match_locations_function': False,
            'embedding_column': False
        }
        
        try:
            # V√©rifier la table locations
            test_query = self.supabase.table('locations').select('id').limit(1).execute()
            verification_results['locations_table'] = True
            
            # V√©rifier les colonnes requises
            # Pour l'instant on assume qu'elles existent si la table existe
            verification_results['embedding_column'] = True
            
        except Exception as e:
            logger.error(f"‚ùå Table 'locations' non trouv√©e: {str(e)}")
            
        try:
            # V√©rifier processed_urls
            self.supabase.table('processed_urls').select('url').limit(1).execute()
            verification_results['processed_urls_table'] = True
        except Exception as e:
            logger.error(f"‚ùå Table 'processed_urls' non trouv√©e: {str(e)}")
            
        try:
            # V√©rifier agent_logs
            self.supabase.table('agent_logs').select('id').limit(1).execute()
            verification_results['agent_logs_table'] = True
        except Exception as e:
            logger.error(f"‚ùå Table 'agent_logs' non trouv√©e: {str(e)}")
            
        # Afficher le r√©sum√©
        all_ok = all(verification_results.values())
        
        if all_ok:
            logger.info("‚úÖ Sch√©ma de base de donn√©es v√©rifi√© - Tout est OK!")
        else:
            logger.error("‚ùå Probl√®mes d√©tect√©s dans le sch√©ma:")
            for check, status in verification_results.items():
                if not status:
                    logger.error(f"  - {check}: MANQUANT")
                    
        return verification_results
        
    def estimate_costs(self, priority='all') -> Dict[str, Any]:
        """Estime les co√ªts avant de lancer le crawl"""
        try:
            # Compter les URLs selon la priorit√©
            url_counts = {
                'high': 0,
                'medium': 0,
                'low': 0
            }
            
            priorities_to_check = []
            if priority == 'all':
                priorities_to_check = ['high', 'medium', 'low']
            elif priority in ['high', 'medium', 'low']:
                priorities_to_check = [priority]
                
            total_urls = 0
            
            for prio in priorities_to_check:
                for sitemap_url in self.PRIORITY_SITEMAPS.get(prio, []):
                    try:
                        response = requests.get(sitemap_url, timeout=30)
                        soup = BeautifulSoup(response.content, 'xml')
                        urls = soup.find_all('loc')
                        url_counts[prio] += len(urls)
                        total_urls += len(urls)
                    except:
                        continue
                        
            # Estimation des POIs (environ 40% sont de vrais POIs)
            estimated_pois = int(total_urls * 0.4)
            
            # Calcul des co√ªts
            scrapingbee_cost = 0  # Gratuit jusqu'√† 1000/mois
            if total_urls > 1000:
                scrapingbee_cost = (total_urls - 1000) * 0.01  # 1 cent par URL extra
                
            openai_cost = total_urls * self.cost_per_url
            total_cost = scrapingbee_cost + openai_cost
            
            # Temps estim√©
            time_per_url = 15  # secondes
            total_time_minutes = (total_urls * time_per_url) / 60
            
            estimation = {
                'url_breakdown': url_counts,
                'total_urls': total_urls,
                'estimated_pois': estimated_pois,
                'costs': {
                    'scrapingbee': f"${scrapingbee_cost:.2f}",
                    'openai': f"${openai_cost:.2f}",
                    'total': f"${total_cost:.2f}"
                },
                'estimated_time': f"{total_time_minutes:.0f} minutes",
                'mode': self.quality_mode,
                'cost_per_poi': f"${total_cost/estimated_pois:.2f}" if estimated_pois > 0 else "N/A"
            }
            
            return estimation
            
        except Exception as e:
            logger.error(f"Erreur lors de l'estimation: {str(e)}")
            return {'error': str(e)}
            
    def log_to_database(self, status: str, message: str, details: Optional[Dict] = None):
        """Enregistre un log dans la table agent_logs"""
        try:
            log_entry = {
                'agent_name': self.agent_name,
                'status': status,
                'message': message,
                'details': details or {}
            }
            
            self.supabase.table('agent_logs').insert(log_entry).execute()
            logger.info(f"üìù Log DB: {status} - {message}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'√©criture du log en DB: {str(e)}")
            
    def fetch_sitemap_urls_by_priority(self, priority='high') -> List[str]:
        """R√©cup√®re les URLs selon la priorit√© choisie"""
        all_urls = []
        sitemaps = []
        
        if priority == 'all':
            for prio in ['high', 'medium', 'low']:
                sitemaps.extend(self.PRIORITY_SITEMAPS[prio])
        else:
            sitemaps = self.PRIORITY_SITEMAPS.get(priority, [])
            
        for sitemap_url in sitemaps:
            try:
                logger.info(f"üì• T√©l√©chargement du sitemap: {sitemap_url}")
                response = requests.get(sitemap_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'xml')
                urls = [loc.text for loc in soup.find_all('loc')]
                
                logger.info(f"‚úÖ {len(urls)} URLs trouv√©es dans {sitemap_url}")
                all_urls.extend(urls)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur pour {sitemap_url}: {str(e)}")
                continue
                
        # D√©dupliquer
        all_urls = list(set(all_urls))
        logger.info(f"üìä Total: {len(all_urls)} URLs uniques (priorit√©: {priority})")
        
        return all_urls
            
    def get_processed_urls(self) -> set:
        """R√©cup√®re la liste des URLs d√©j√† trait√©es depuis la DB"""
        try:
            response = self.supabase.table('processed_urls').select('url').execute()
            processed_urls = {row['url'] for row in response.data}
            
            logger.info(f"üìä {len(processed_urls)} URLs d√©j√† trait√©es en base")
            return processed_urls
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des URLs trait√©es: {str(e)}")
            return set()
            
    def download_page_content(self, url: str) -> str:
        """T√©l√©charge le contenu HTML d'une page via ScrapingBee"""
        try:
            params = {
                'api_key': self.scrapingbee_api_key,
                'url': url,
                'render_js': 'false',
                'block_resources': 'true',
                'premium_proxy': 'false'
            }
            
            response = requests.get('https://app.scrapingbee.com/api/v1/', params=params, timeout=60)
            response.raise_for_status()
            
            return response.text
            
        except Exception as e:
            logger.error(f"‚ùå Erreur ScrapingBee pour {url}: {str(e)}")
            raise
            
    def extract_text_content(self, html: str) -> str:
        """Extrait le texte principal d'une page HTML Tokyo Cheapo"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Suppression des √©l√©ments non pertinents
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
                
            # Structure sp√©cifique Tokyo Cheapo
            # 1. Chercher le contenu principal avec les bonnes classes
            main_content = (
                soup.find('div', class_='entry-content') or 
                soup.find('article') or 
                soup.find('main') or
                soup.find('div', class_='article')
            )
            
            if main_content:
                text = main_content.get_text(separator=' ', strip=True)
            else:
                text = soup.get_text(separator=' ', strip=True)
                
            # Nettoyage
            text = ' '.join(text.split())
            
            # Limitation pour les APIs
            if len(text) > 10000:
                text = text[:10000] + "..."
                
            return text
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction du texte: {str(e)}")
            raise
            
    def classify_as_poi(self, text: str) -> Tuple[bool, str]:
        """Classification intelligente avec cat√©gorisation"""
        try:
            prompt = """Tu es un expert en voyage √† Tokyo.
Analyse ce texte et d√©termine:
1. Est-ce un lieu physique unique et visitable √† Tokyo ? (OUI/NON)
2. Si OUI, quelle cat√©gorie : RESTAURANT, ATTRACTION, HOTEL, SHOPPING, TEMPLE, MUSEE, PARC, BAR, CAFE, AUTRE

R√©ponds UNIQUEMENT au format JSON: {"is_poi": true/false, "category": "CATEGORIE"}"""
            
            response = self.openai_client.chat.completions.create(
                model=self.classification_model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text[:2500]}
                ],
                temperature=0.3,
                max_tokens=50
            )
            
            try:
                result = json.loads(response.choices[0].message.content)
                is_poi = result.get('is_poi', False)
                category = result.get('category', 'AUTRE')
            except:
                # Fallback si pas de JSON valide
                answer = response.choices[0].message.content.upper()
                is_poi = 'OUI' in answer or 'TRUE' in answer
                category = 'AUTRE'
                
            # Co√ªt
            if self.classification_model == "gpt-4-turbo-preview":
                self.total_cost_estimate += 0.03
            else:
                self.total_cost_estimate += 0.002
                
            return is_poi, category
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la classification: {str(e)}")
            return False, 'UNKNOWN'
            
    def generate_quality_description(self, text: str, category: str) -> str:
        """G√©n√®re une description de qualit√© selon le mode"""
        try:
            # Prompts adapt√©s par cat√©gorie
            if category == 'RESTAURANT':
                style = "culinaire et app√©tissant"
            elif category in ['TEMPLE', 'MUSEE']:
                style = "culturel et respectueux"
            elif category == 'ATTRACTION':
                style = "captivant et informatif"
            else:
                style = "engageant et descriptif"
                
            prompt = f"""Tu es un r√©dacteur expert de guides de voyage, sp√©cialiste de Tokyo.
Cr√©e une description {style} de ce lieu en 150-200 mots.
La description doit √™tre 100% originale, captivante et donner envie de visiter.
Ne copie AUCUNE phrase du texte source."""
            
            response = self.openai_client.chat.completions.create(
                model=self.generation_model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text[:3500]}
                ],
                temperature=0.8,
                max_tokens=400,
                presence_penalty=0.5
            )
            
            description = response.choices[0].message.content.strip()
            
            # Co√ªt
            if self.generation_model == "gpt-4-turbo-preview":
                self.total_cost_estimate += 0.04
            else:
                self.total_cost_estimate += 0.004
                
            return description
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration: {str(e)}")
            raise
            
    def extract_structured_data(self, text: str, description: str, category: str) -> Dict[str, Any]:
        """Extraction de donn√©es structur√©es"""
        try:
            prompt = """Extrais les informations suivantes au format JSON.
Si une information est introuvable, retourne null.

{
  "name": "Nom du lieu",
  "neighborhood": "Quartier de Tokyo",
  "summary": "R√©sum√© en 1 phrase (max 100 char)",
  "keywords": ["5-8 mots-cl√©s pertinents"],
  "price_range": "¬• √† ¬•¬•¬•¬•¬• ou null"
}"""
            
            context = f"Cat√©gorie: {category}\nDescription: {description}\nTexte source: {text[:1500]}"
            
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",  # Toujours 3.5 pour l'extraction
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": context}
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            try:
                extracted = json.loads(response.choices[0].message.content)
            except:
                extracted = {"name": None, "summary": None, "keywords": []}
                
            self.total_cost_estimate += 0.003
            
            return extracted
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction: {str(e)}")
            return {"name": None, "summary": None, "keywords": []}
            
    def get_embedding(self, text: str) -> List[float]:
        """G√©n√®re un embedding pour le texte"""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=text[:8000]
            )
            
            self.total_cost_estimate += 0.0004
            
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration d'embedding: {str(e)}")
            raise
            
    def check_semantic_duplicate(self, embedding: List[float], threshold: float = 0.9) -> bool:
        """V√©rifie si un POI similaire existe d√©j√†"""
        try:
            response = self.supabase.rpc(
                'match_locations',
                {
                    'query_embedding': embedding,
                    'match_threshold': threshold,
                    'match_count': 1
                }
            ).execute()
            
            return len(response.data) > 0
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors de la v√©rification de doublon: {str(e)}")
            return False
            
    def save_poi_to_database(self, poi_data: Dict[str, Any], url: str, category: str) -> bool:
        """Sauvegarde un POI dans la base de donn√©es"""
        try:
            location_data = {
                'name': poi_data['name'] or "POI sans nom",
                'description': poi_data['description'],
                'summary': poi_data.get('summary'),
                'category': category,
                'neighborhood': poi_data.get('neighborhood'),
                'is_active': False,
                'source_url': url,
                'source_name': self.site_name,
                'source_scraped_at': datetime.utcnow().isoformat(),
                'embedding': poi_data.get('embedding'),
                'features': {
                    'keywords': poi_data.get('keywords', []),
                    'price_range': poi_data.get('price_range'),
                    'quality_mode': self.quality_mode
                }
            }
            
            self.supabase.table('locations').insert(location_data).execute()
            
            logger.info(f"‚úÖ POI cr√©√©: {poi_data['name']} ({category})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde: {str(e)}")
            raise
            
    def mark_url_as_processed(self, url: str, status: str):
        """Marque une URL comme trait√©e"""
        try:
            self.supabase.table('processed_urls').insert({
                'url': url,
                'status': status
            }).execute()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du marquage de l'URL: {str(e)}")
            
    def process_single_url(self, url: str) -> str:
        """Traite une URL unique avec strat√©gie smart"""
        try:
            logger.info(f"üîÑ Traitement de: {url}")
            
            # √âtape 1: T√©l√©chargement
            html_content = self.download_page_content(url)
            text_content = self.extract_text_content(html_content)
            
            if len(text_content) < 200:
                logger.warning(f"‚ö†Ô∏è Contenu trop court pour {url}")
                return 'skipped_not_a_poi'
                
            # √âtape 2: Classification (toujours avec le mod√®le configur√©)
            is_poi, category = self.classify_as_poi(text_content)
            
            if not is_poi:
                logger.info(f"‚ÑπÔ∏è {url} n'est pas un POI")
                return 'skipped_not_a_poi'
                
            logger.info(f"‚úÖ POI d√©tect√© - Cat√©gorie: {category}")
            
            # √âtape 3: G√©n√©ration de description (avec le bon mod√®le selon mode)
            description = self.generate_quality_description(text_content, category)
            
            # √âtape 4: Extraction structur√©e
            structured_data = self.extract_structured_data(text_content, description, category)
            
            # √âtape 5: Embedding
            embedding = self.get_embedding(description)
            
            # √âtape 6: V√©rification doublon
            is_duplicate = self.check_semantic_duplicate(embedding)
            
            if is_duplicate:
                logger.info(f"‚ÑπÔ∏è Doublon d√©tect√© pour {url}")
                return 'skipped_duplicate'
                
            # √âtape 7: Sauvegarde
            poi_data = {
                **structured_data,
                'description': description,
                'embedding': embedding
            }
            
            self.save_poi_to_database(poi_data, url, category)
            
            return 'success'
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement de {url}: {str(e)}")
            logger.error(traceback.format_exc())
            return 'failed'
            
    def run(self, priority='high', dry_run=False):
        """
        M√©thode principale d'ex√©cution
        
        Args:
            priority: 'high', 'medium', 'low', ou 'all'
            dry_run: Si True, affiche seulement l'estimation sans crawler
        """
        try:
            # V√©rification du sch√©ma
            logger.info("üîç V√©rification du sch√©ma de base de donn√©es...")
            schema_check = self.verify_database_schema()
            
            if not all(schema_check.values()):
                logger.error("‚ùå Le sch√©ma de base de donn√©es n'est pas complet!")
                logger.error("Ex√©cutez d'abord le script database_setup.sql dans Supabase")
                return
                
            # Estimation des co√ªts
            logger.info(f"üí∞ Estimation des co√ªts (mode: {self.quality_mode}, priorit√©: {priority})...")
            estimation = self.estimate_costs(priority)
            
            print("\n" + "="*60)
            print("üìä ESTIMATION DU CRAWL")
            print("="*60)
            print(f"Mode de qualit√©: {self.quality_mode.upper()}")
            print(f"Priorit√©: {priority}")
            print(f"URLs √† traiter: {estimation['total_urls']}")
            print(f"POIs estim√©s: {estimation['estimated_pois']}")
            print(f"Temps estim√©: {estimation['estimated_time']}")
            print(f"Co√ªt total estim√©: {estimation['costs']['total']}")
            print(f"Co√ªt par POI: {estimation['cost_per_poi']}")
            print("="*60 + "\n")
            
            if dry_run:
                logger.info("Mode DRY RUN - Arr√™t ici")
                return
                
            # Confirmation
            if estimation['total_urls'] > 100:
                confirm = input(f"‚ö†Ô∏è  Voulez-vous vraiment crawler {estimation['total_urls']} URLs? (oui/non): ")
                if confirm.lower() != 'oui':
                    logger.info("Crawl annul√©")
                    return
                    
            # Log de d√©marrage
            self.log_to_database(
                'STARTED',
                f'Lancement du crawl SMART (mode: {self.quality_mode}, priorit√©: {priority})',
                estimation
            )
            
            # R√©cup√©ration des URLs
            logger.info("üöÄ D√©marrage du crawler intelligent")
            all_urls = self.fetch_sitemap_urls_by_priority(priority)
            processed_urls = self.get_processed_urls()
            
            # URLs √† traiter
            urls_to_process = [url for url in all_urls if url not in processed_urls]
            
            if not urls_to_process:
                message = "Aucune nouvelle URL √† traiter."
                logger.info(f"‚úÖ {message}")
                self.log_to_database('SUCCESS', message)
                return
                
            logger.info(f"üìã {len(urls_to_process)} nouvelles URLs √† traiter")
            
            # Boucle de traitement
            start_time = time.time()
            
            for idx, url in enumerate(urls_to_process, 1):
                try:
                    # Traitement
                    status = self.process_single_url(url)
                    
                    # Mise √† jour des compteurs
                    self.processed_count += 1
                    if status == 'success':
                        self.success_count += 1
                    elif status in ['skipped_not_a_poi', 'skipped_duplicate']:
                        self.skip_count += 1
                    else:
                        self.error_count += 1
                        
                    # Enregistrement
                    self.mark_url_as_processed(url, status)
                    
                    # Log de progression
                    if self.processed_count % self.batch_log_interval == 0:
                        elapsed = time.time() - start_time
                        rate = self.processed_count / (elapsed / 60)
                        
                        progress_msg = (
                            f"Progression: {self.processed_count}/{len(urls_to_process)} - "
                            f"‚úÖ {self.success_count} POIs, "
                            f"‚è≠Ô∏è {self.skip_count} ignor√©s, "
                            f"‚ùå {self.error_count} erreurs - "
                            f"Vitesse: {rate:.1f} URLs/min - "
                            f"Co√ªt: ${self.total_cost_estimate:.2f}"
                        )
                        logger.info(progress_msg)
                        self.log_to_database('RUNNING', progress_msg)
                        
                    # D√©lai
                    time.sleep(self.delay_between_urls)
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur non g√©r√©e pour {url}: {str(e)}")
                    self.error_count += 1
                    self.mark_url_as_processed(url, 'failed')
                    continue
                    
            # Log final
            duration = time.time() - start_time
            final_message = (
                f"Scan termin√©! Total: {self.processed_count}, "
                f"POIs cr√©√©s: {self.success_count}, "
                f"Ignor√©s: {self.skip_count}, "
                f"Erreurs: {self.error_count} - "
                f"Dur√©e: {duration/60:.1f} min - "
                f"Co√ªt total: ${self.total_cost_estimate:.2f}"
            )
            
            logger.info(f"üéâ {final_message}")
            self.log_to_database('SUCCESS', final_message, {
                'total_processed': self.processed_count,
                'success': self.success_count,
                'skipped': self.skip_count,
                'errors': self.error_count,
                'duration_seconds': duration,
                'total_cost': self.total_cost_estimate
            })
            
        except Exception as e:
            error_msg = f"Erreur fatale: {str(e)}"
            logger.error(f"üí• {error_msg}")
            logger.error(traceback.format_exc())
            self.log_to_database('ERROR', error_msg)
            raise


def main():
    """Point d'entr√©e avec options de ligne de commande"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Yorimichi Smart Crawler')
    parser.add_argument(
        '--mode', 
        choices=['smart', 'premium', 'economy'], 
        default='smart',
        help='Mode de qualit√© (smart recommand√©)'
    )
    parser.add_argument(
        '--priority',
        choices=['high', 'medium', 'low', 'all'],
        default='high',
        help='Priorit√© des sitemaps √† crawler'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Affiche seulement l\'estimation sans crawler'
    )
    
    args = parser.parse_args()
    
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           YORIMICHI SMART CRAWLER - Version 3.0              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    try:
        crawler = YorimichiSmartCrawler(quality_mode=args.mode)
        crawler.run(priority=args.priority, dry_run=args.dry_run)
        
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è Arr√™t manuel du crawler")
        sys.exit(0)
    except Exception as e:
        logger.error(f"üí• Erreur fatale: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()