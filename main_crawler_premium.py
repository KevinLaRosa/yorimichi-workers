#!/usr/bin/env python3
"""
Worker Intelligent Yorimichi V2 - PREMIUM QUALITY EDITION
Version optimis√©e pour la meilleure qualit√© de contenu possible
"""

import os
import sys
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import traceback

# Imports externes
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from supabase import create_client, Client
import openai
from openai import OpenAI

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('yorimichi_crawler_premium.log')
    ]
)
logger = logging.getLogger('YorimichiPremiumCrawler')


class YorimichiPremiumCrawler:
    """Agent premium de collecte avec focus sur la qualit√© maximale"""
    
    # Tous les sitemaps de Tokyo Cheapo √† crawler
    TOKYO_CHEAPO_SITEMAPS = [
        "https://tokyocheapo.com/place-sitemap1.xml",
        "https://tokyocheapo.com/place-sitemap2.xml",
        "https://tokyocheapo.com/place-sitemap3.xml",
        "https://tokyocheapo.com/restaurant-sitemap1.xml",
        "https://tokyocheapo.com/restaurant-sitemap2.xml",
        "https://tokyocheapo.com/accommodation-sitemap.xml",
        "https://tokyocheapo.com/event-sitemap1.xml",
        "https://tokyocheapo.com/event-sitemap2.xml",
        "https://tokyocheapo.com/tour-sitemap.xml",
    ]
    
    def __init__(self):
        """Initialisation du crawler premium avec configuration de qualit√©"""
        # Charger les variables d'environnement
        if os.path.exists('.env.local'):
            load_dotenv('.env.local')
        else:
            load_dotenv()
        
        # Validation des variables d'environnement
        self.validate_environment()
        
        # Initialisation des clients
        self.supabase = create_client(
            os.getenv('NEXT_PUBLIC_SUPABASE_URL'),
            os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        )
        
        # Configuration OpenAI avec mod√®les premium
        openai.api_key = os.getenv('OPENAI_API_KEY')
        self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # Configuration ScrapingBee
        self.scrapingbee_api_key = os.getenv('SCRAPINGBEE_API_KEY')
        
        # Param√®tres de configuration PREMIUM
        self.site_name = "Tokyo Cheapo"
        self.agent_name = "Premium Intelligent Crawler V2"
        self.batch_log_interval = 10
        self.delay_between_urls = 2  # Plus de d√©lai pour √©viter rate limits
        
        # Mod√®les GPT premium
        self.classification_model = "gpt-4-turbo-preview"  # GPT-4 m√™me pour classification
        self.generation_model = "gpt-4-turbo-preview"
        self.embedding_model = "text-embedding-3-large"  # Meilleur mod√®le d'embedding
        
        # Compteurs pour le suivi
        self.processed_count = 0
        self.success_count = 0
        self.skip_count = 0
        self.error_count = 0
        self.total_cost_estimate = 0.0
        
    def validate_environment(self):
        """Valide que toutes les variables d'environnement requises sont pr√©sentes"""
        required_vars = [
            'NEXT_PUBLIC_SUPABASE_URL',
            'SUPABASE_SERVICE_ROLE_KEY',
            'OPENAI_API_KEY',
            'SCRAPINGBEE_API_KEY'
        ]
        
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            error_msg = f"Variables d'environnement manquantes: {', '.join(missing_vars)}"
            logger.error(error_msg)
            raise EnvironmentError(error_msg)
            
        logger.info("‚úÖ Toutes les variables d'environnement sont configur√©es")
        
    def log_to_database(self, status: str, message: str, details: Optional[Dict] = None):
        """Enregistre un log dans la table agent_logs"""
        try:
            log_entry = {
                'agent_name': self.agent_name,
                'status': status,
                'message': message,
                'details': details or {}
            }
            
            self.supabase.table('agent_logs').insert(log_entry).execute()
            logger.info(f"üìù Log DB: {status} - {message}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'√©criture du log en DB: {str(e)}")
            
    def fetch_all_sitemap_urls(self) -> List[str]:
        """R√©cup√®re TOUTES les URLs depuis TOUS les sitemaps Tokyo Cheapo"""
        all_urls = []
        
        for sitemap_url in self.TOKYO_CHEAPO_SITEMAPS:
            try:
                logger.info(f"üì• T√©l√©chargement du sitemap: {sitemap_url}")
                
                response = requests.get(sitemap_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'xml')
                urls = [loc.text for loc in soup.find_all('loc')]
                
                logger.info(f"‚úÖ {len(urls)} URLs trouv√©es dans {sitemap_url}")
                all_urls.extend(urls)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur pour {sitemap_url}: {str(e)}")
                continue
                
        # D√©dupliquer au cas o√π
        all_urls = list(set(all_urls))
        logger.info(f"üìä Total: {len(all_urls)} URLs uniques trouv√©es")
        
        return all_urls
            
    def get_processed_urls(self) -> set:
        """R√©cup√®re la liste des URLs d√©j√† trait√©es depuis la DB"""
        try:
            response = self.supabase.table('processed_urls').select('url').execute()
            processed_urls = {row['url'] for row in response.data}
            
            logger.info(f"üìä {len(processed_urls)} URLs d√©j√† trait√©es en base")
            return processed_urls
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des URLs trait√©es: {str(e)}")
            return set()
            
    def download_page_content(self, url: str) -> str:
        """T√©l√©charge le contenu HTML d'une page via ScrapingBee avec options premium"""
        try:
            params = {
                'api_key': self.scrapingbee_api_key,
                'url': url,
                'render_js': 'false',
                'block_resources': 'false',  # On veut tout le contenu
                'premium_proxy': 'true',  # Proxy premium pour meilleure fiabilit√©
                'country_code': 'jp'  # Depuis le Japon pour contenu local
            }
            
            response = requests.get('https://app.scrapingbee.com/api/v1/', params=params, timeout=60)
            response.raise_for_status()
            
            return response.text
            
        except Exception as e:
            logger.error(f"‚ùå Erreur ScrapingBee pour {url}: {str(e)}")
            raise
            
    def extract_rich_content(self, html: str, url: str) -> Dict[str, Any]:
        """Extraction enrichie du contenu avec m√©tadonn√©es"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extraction enrichie avec plus de contexte
            content = {
                'text': '',
                'title': '',
                'meta_description': '',
                'images': [],
                'categories': [],
                'tags': [],
                'address': None,
                'opening_hours': None,
                'price_info': None
            }
            
            # Titre
            title_tag = soup.find('h1') or soup.find('title')
            if title_tag:
                content['title'] = title_tag.get_text(strip=True)
                
            # Meta description
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc:
                content['meta_description'] = meta_desc.get('content', '')
                
            # Images principales
            images = soup.find_all('img', limit=5)
            content['images'] = [img.get('src', '') for img in images if img.get('src')]
            
            # Cat√©gories et tags
            categories = soup.find_all(['a'], {'rel': 'category tag'})
            content['categories'] = [cat.get_text(strip=True) for cat in categories]
            
            tags = soup.find_all(['a'], {'rel': 'tag'})
            content['tags'] = [tag.get_text(strip=True) for tag in tags]
            
            # Contenu principal avec structure pr√©serv√©e
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
            
            if main_content:
                # Garder les titres pour la structure
                for tag in ['h2', 'h3', 'h4']:
                    for header in main_content.find_all(tag):
                        header.string = f"\n\n[{tag.upper()}] {header.get_text(strip=True)}\n"
                        
                # Extraire le texte structur√©
                content['text'] = main_content.get_text(separator=' ', strip=True)
            else:
                content['text'] = soup.get_text(separator=' ', strip=True)
                
            # Nettoyage mais en gardant la richesse
            content['text'] = ' '.join(content['text'].split())
            
            # Informations sp√©cifiques si d√©tectables
            address_elem = soup.find(['address', 'div'], class_=['address', 'location'])
            if address_elem:
                content['address'] = address_elem.get_text(strip=True)
                
            # Prix
            price_elems = soup.find_all(text=lambda t: '¬•' in t or 'yen' in t.lower())
            if price_elems:
                content['price_info'] = ' | '.join([p.strip() for p in price_elems[:3]])
                
            return content
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction enrichie: {str(e)}")
            raise
            
    def classify_as_poi_premium(self, content: Dict[str, Any]) -> Tuple[bool, str]:
        """Classification premium avec cat√©gorisation"""
        try:
            # Prompt am√©lior√© pour classification ET cat√©gorisation
            prompt = """Tu es un expert en voyage √† Tokyo avec 10 ans d'exp√©rience.
            
Analyse ce contenu et d√©termine:
1. Est-ce un lieu physique unique et visitable √† Tokyo ? (OUI/NON)
2. Si OUI, quelle cat√©gorie principale : RESTAURANT, ATTRACTION, HOTEL, SHOPPING, TEMPLE, MUSEE, PARC, BAR, CAFE, AUTRE

R√©ponds au format JSON: {"is_poi": true/false, "category": "CATEGORIE"}

IMPORTANT: Un article de blog g√©n√©ral ou un guide n'est PAS un POI."""
            
            # Contexte enrichi pour la classification
            context_text = f"""
Titre: {content['title']}
Description: {content['meta_description']}
Cat√©gories: {', '.join(content['categories'])}
Contenu: {content['text'][:3000]}
"""
            
            response = self.openai_client.chat.completions.create(
                model=self.classification_model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": context_text}
                ],
                temperature=0.1,  # Tr√®s faible pour coh√©rence
                max_tokens=50,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # Estimation du co√ªt
            self.total_cost_estimate += 0.03  # GPT-4 classification
            
            return result.get('is_poi', False), result.get('category', 'AUTRE')
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la classification premium: {str(e)}")
            return False, 'UNKNOWN'
            
    def generate_premium_description(self, content: Dict[str, Any], category: str) -> str:
        """G√©n√©ration de description premium adapt√©e √† la cat√©gorie"""
        try:
            # Prompts sp√©cialis√©s par cat√©gorie
            category_prompts = {
                'RESTAURANT': """Tu es Anthony Bourdain r√©incarn√©, expert culinaire passionn√© par Tokyo.
Cr√©e une description qui capture l'essence de ce restaurant : l'ambiance, les saveurs signature, 
l'exp√©rience unique. √âvoque les √©motions, les parfums, la magie du lieu.
Style: Po√©tique mais authentique, 150-200 mots.""",
                
                'ATTRACTION': """Tu es un guide culturel expert de Tokyo depuis 20 ans.
D√©cris cette attraction en capturant son essence unique, son importance culturelle,
et pourquoi elle m√©rite d'√™tre visit√©e. Transmets l'√©merveillement.
Style: Inspirant et informatif, 150-200 mots.""",
                
                'TEMPLE': """Tu es un expert en culture japonaise et spiritualit√©.
D√©cris ce temple en √©voquant sa s√©r√©nit√©, son histoire, son architecture.
Aide le lecteur √† ressentir la paix et la beaut√© sacr√©e du lieu.
Style: Respectueux et contemplatif, 150-200 mots.""",
                
                'DEFAULT': """Tu es un √©crivain de guides de voyage prim√©, sp√©cialiste de Tokyo.
Cr√©e une description captivante et unique de ce lieu. √âvoque les sensations,
l'atmosph√®re, ce qui rend cet endroit sp√©cial et m√©morable.
Style: Engageant et √©vocateur, 150-200 mots."""
            }
            
            prompt = category_prompts.get(category, category_prompts['DEFAULT'])
            
            # Contexte enrichi pour la g√©n√©ration
            enriched_context = f"""
Titre: {content['title']}
Cat√©gorie: {category}
Adresse: {content['address'] or 'Non sp√©cifi√©e'}
Prix: {content['price_info'] or 'Non sp√©cifi√©'}
Tags: {', '.join(content['tags'])}

Contenu source:
{content['text'][:4000]}

CONSIGNES CRITIQUES:
- Ne JAMAIS copier de phrases du texte source
- Cr√©er une description 100% originale et unique
- Capturer l'essence √©motionnelle du lieu
- Utiliser un vocabulaire riche et √©vocateur
- Rester authentique et cr√©dible
"""
            
            response = self.openai_client.chat.completions.create(
                model=self.generation_model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": enriched_context}
                ],
                temperature=0.85,  # Cr√©ativit√© √©lev√©e
                max_tokens=400,
                presence_penalty=0.6,  # √âviter les r√©p√©titions
                frequency_penalty=0.4
            )
            
            description = response.choices[0].message.content.strip()
            
            # Estimation du co√ªt
            self.total_cost_estimate += 0.04  # GPT-4 g√©n√©ration
            
            return description
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration premium: {str(e)}")
            raise
            
    def extract_structured_data_premium(self, content: Dict[str, Any], description: str, category: str) -> Dict[str, Any]:
        """Extraction de donn√©es structur√©es enrichies"""
        try:
            prompt = f"""En tant qu'expert data analyst sp√©cialis√© dans le tourisme √† Tokyo,
extrais les informations structur√©es suivantes au format JSON.

Cat√©gorie du lieu: {category}

Si une information est introuvable, retourne null. Sois pr√©cis et factuel.

{{
  "name": "Nom officiel du lieu",
  "name_jp": "Nom en japonais si disponible",
  "neighborhood": "Quartier de Tokyo",
  "district": "Arrondissement (ex: Shibuya-ku)",
  "summary": "R√©sum√© factuel en 1 phrase (max 100 caract√®res)",
  "keywords": ["5-8 mots-cl√©s pertinents"],
  "price_range": "¬• √† ¬•¬•¬•¬•¬• ou null",
  "best_for": ["couples", "familles", "solo", "groupes", etc.],
  "highlights": ["3-5 points forts uniques"],
  "practical_info": {{
    "nearest_station": "Station la plus proche",
    "walking_time": "Temps de marche depuis la station",
    "best_time_to_visit": "Meilleur moment pour visiter"
  }}
}}"""
            
            # Contexte complet pour l'extraction
            full_context = f"""
Description g√©n√©r√©e: {description}

Informations source:
Titre: {content['title']}
Adresse: {content['address']}
Prix: {content['price_info']}
Cat√©gories: {', '.join(content['categories'])}
Tags: {', '.join(content['tags'])}

Texte original (extrait):
{content['text'][:2000]}
"""
            
            response = self.openai_client.chat.completions.create(
                model=self.generation_model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": full_context}
                ],
                temperature=0.2,  # Pr√©cision pour l'extraction
                max_tokens=500,
                response_format={"type": "json_object"}
            )
            
            extracted_data = json.loads(response.choices[0].message.content)
            
            # Estimation du co√ªt
            self.total_cost_estimate += 0.03  # GPT-4 extraction
            
            return extracted_data
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erreur de parsing JSON: {str(e)}")
            return {"name": content['title'], "summary": None, "keywords": []}
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction premium: {str(e)}")
            raise
            
    def get_premium_embedding(self, text: str) -> List[float]:
        """G√©n√®re un embedding de haute qualit√© pour le texte"""
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text[:8000]
            )
            
            # Estimation du co√ªt
            self.total_cost_estimate += 0.0013  # Embedding large model
            
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration d'embedding: {str(e)}")
            raise
            
    def check_semantic_duplicate(self, embedding: List[float], threshold: float = 0.92) -> bool:
        """V√©rifie les doublons avec seuil plus strict pour qualit√©"""
        try:
            response = self.supabase.rpc(
                'match_locations',
                {
                    'query_embedding': embedding,
                    'match_threshold': threshold,
                    'match_count': 3  # V√©rifier les 3 plus proches
                }
            ).execute()
            
            # Log si on trouve des matchs proches
            if response.data:
                for match in response.data:
                    logger.info(f"üìä Match trouv√©: {match['name']} (similarit√©: {match['similarity']:.3f})")
            
            return len(response.data) > 0
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors de la v√©rification de doublon: {str(e)}")
            return False
            
    def save_premium_poi_to_database(self, poi_data: Dict[str, Any], url: str, category: str, content: Dict[str, Any]) -> bool:
        """Sauvegarde un POI premium avec toutes les m√©tadonn√©es enrichies"""
        try:
            # Structure enrichie pour la base de donn√©es
            location_data = {
                'name': poi_data['extracted']['name'] or poi_data['title'],
                'name_jp': poi_data['extracted'].get('name_jp'),
                'description': poi_data['description'],
                'summary': poi_data['extracted'].get('summary'),
                'category': category,
                'neighborhood': poi_data['extracted'].get('neighborhood'),
                'district': poi_data['extracted'].get('district'),
                'is_active': False,  # Brouillon par d√©faut
                'source_url': url,
                'source_name': self.site_name,
                'source_scraped_at': datetime.utcnow().isoformat(),
                'embedding': poi_data['embedding'],
                
                # Features enrichies
                'features': {
                    'keywords': poi_data['extracted'].get('keywords', []),
                    'price_range': poi_data['extracted'].get('price_range'),
                    'best_for': poi_data['extracted'].get('best_for', []),
                    'highlights': poi_data['extracted'].get('highlights', []),
                    'practical_info': poi_data['extracted'].get('practical_info', {}),
                    'original_categories': content['categories'],
                    'original_tags': content['tags'],
                    'images': content['images'][:3],  # Top 3 images
                    'quality_score': 0.95  # Score de qualit√© premium
                },
                
                # M√©tadonn√©es de g√©n√©ration
                'generation_metadata': {
                    'crawler_version': 'Premium V2',
                    'models_used': {
                        'classification': self.classification_model,
                        'generation': self.generation_model,
                        'embedding': self.embedding_model
                    },
                    'generation_date': datetime.utcnow().isoformat()
                }
            }
            
            # Si on a une adresse, l'ajouter
            if content.get('address'):
                location_data['address'] = content['address']
                
            # Insertion dans la table locations
            self.supabase.table('locations').insert(location_data).execute()
            
            logger.info(f"üåü POI PREMIUM cr√©√©: {location_data['name']} ({category})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde du POI premium: {str(e)}")
            raise
            
    def mark_url_as_processed(self, url: str, status: str, error_details: str = None):
        """Marque une URL comme trait√©e avec d√©tails optionnels"""
        try:
            data = {
                'url': url,
                'status': status,
                'processed_at': datetime.utcnow().isoformat()
            }
            
            if error_details:
                data['error_details'] = error_details
                
            self.supabase.table('processed_urls').insert(data).execute()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du marquage de l'URL: {str(e)}")
            
    def process_single_url_premium(self, url: str) -> str:
        """Traite une URL unique avec pipeline de qualit√© premium"""
        try:
            logger.info(f"üîÑ Traitement PREMIUM de: {url}")
            
            # √âtape 1: T√©l√©chargement du contenu enrichi
            html_content = self.download_page_content(url)
            rich_content = self.extract_rich_content(html_content, url)
            
            if len(rich_content['text']) < 200:
                logger.warning(f"‚ö†Ô∏è Contenu trop court pour {url}")
                return 'skipped_not_a_poi'
                
            # √âtape 2: Classification premium avec cat√©gorisation
            is_poi, category = self.classify_as_poi_premium(rich_content)
            
            if not is_poi:
                logger.info(f"‚ÑπÔ∏è {url} n'est pas un POI")
                return 'skipped_not_a_poi'
                
            logger.info(f"‚úÖ POI d√©tect√© - Cat√©gorie: {category}")
            
            # √âtape 3: G√©n√©ration de description premium
            lovely_description = self.generate_premium_description(rich_content, category)
            logger.info(f"‚ú® Description premium g√©n√©r√©e ({len(lovely_description)} caract√®res)")
            
            # √âtape 4: Extraction de donn√©es structur√©es enrichies
            structured_data = self.extract_structured_data_premium(
                rich_content, 
                lovely_description, 
                category
            )
            
            # √âtape 5: G√©n√©ration d'embedding haute qualit√©
            embedding = self.get_premium_embedding(lovely_description)
            
            # √âtape 6: V√©rification stricte des doublons
            is_duplicate = self.check_semantic_duplicate(embedding)
            
            if is_duplicate:
                logger.info(f"‚ÑπÔ∏è Doublon d√©tect√© pour {url}")
                return 'skipped_duplicate'
                
            # √âtape 7: Sauvegarde enrichie
            poi_data = {
                'title': rich_content['title'],
                'description': lovely_description,
                'extracted': structured_data,
                'embedding': embedding
            }
            
            self.save_premium_poi_to_database(poi_data, url, category, rich_content)
            
            return 'success'
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement premium de {url}: {str(e)}")
            logger.error(traceback.format_exc())
            return 'failed'
            
    def run(self):
        """M√©thode principale d'ex√©cution du crawler premium"""
        try:
            # Log de d√©marrage
            start_time = time.time()
            self.log_to_database(
                'STARTED',
                f'üåü Lancement du scan PREMIUM pour {self.site_name} (Tous les sitemaps)',
                {'sitemaps_count': len(self.TOKYO_CHEAPO_SITEMAPS)}
            )
            
            # R√©cup√©ration de TOUTES les URLs
            logger.info("üöÄ D√©marrage du crawler PREMIUM Yorimichi")
            all_urls = self.fetch_all_sitemap_urls()
            processed_urls = self.get_processed_urls()
            
            # Calcul des URLs √† traiter
            urls_to_process = [url for url in all_urls if url not in processed_urls]
            
            if not urls_to_process:
                message = "Aucune nouvelle URL √† traiter."
                logger.info(f"‚úÖ {message}")
                self.log_to_database('SUCCESS', message)
                return
                
            logger.info(f"üìã {len(urls_to_process)} nouvelles URLs √† traiter en mode PREMIUM")
            estimated_cost = len(urls_to_process) * 0.10  # ~0.10$ par URL en premium
            logger.info(f"üí∞ Co√ªt estim√©: ~${estimated_cost:.2f}")
            
            # Boucle de traitement principale
            for idx, url in enumerate(urls_to_process, 1):
                try:
                    # Traitement premium de l'URL
                    status = self.process_single_url_premium(url)
                    
                    # Mise √† jour des compteurs
                    self.processed_count += 1
                    if status == 'success':
                        self.success_count += 1
                    elif status in ['skipped_not_a_poi', 'skipped_duplicate']:
                        self.skip_count += 1
                    else:
                        self.error_count += 1
                        
                    # Enregistrement du statut
                    self.mark_url_as_processed(url, status)
                    
                    # Log de progression d√©taill√©
                    if self.processed_count % self.batch_log_interval == 0:
                        elapsed_time = time.time() - start_time
                        rate = self.processed_count / (elapsed_time / 60)  # URLs par minute
                        remaining = len(urls_to_process) - self.processed_count
                        eta_minutes = remaining / rate if rate > 0 else 0
                        
                        progress_msg = (
                            f"üéØ Progression: {self.processed_count}/{len(urls_to_process)} URLs\n"
                            f"‚úÖ {self.success_count} POIs PREMIUM cr√©√©s\n"
                            f"‚è≠Ô∏è {self.skip_count} ignor√©s\n"
                            f"‚ùå {self.error_count} erreurs\n"
                            f"‚è±Ô∏è Vitesse: {rate:.1f} URLs/min\n"
                            f"‚è≥ ETA: {eta_minutes:.0f} minutes\n"
                            f"üí∞ Co√ªt actuel: ~${self.total_cost_estimate:.2f}"
                        )
                        logger.info(progress_msg)
                        self.log_to_database('RUNNING', progress_msg, {
                            'processed': self.processed_count,
                            'total': len(urls_to_process),
                            'success': self.success_count,
                            'skipped': self.skip_count,
                            'errors': self.error_count,
                            'cost_estimate': self.total_cost_estimate,
                            'rate_per_minute': rate
                        })
                        
                    # D√©lai entre les requ√™tes (plus long pour √©viter rate limits)
                    time.sleep(self.delay_between_urls)
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur non g√©r√©e pour {url}: {str(e)}")
                    self.error_count += 1
                    self.mark_url_as_processed(url, 'failed', str(e))
                    self.log_to_database('ERROR', f"Erreur sur {url}", {'error': str(e)})
                    continue
                    
            # Statistiques finales
            total_time = time.time() - start_time
            final_message = (
                f"üåü Scan PREMIUM termin√© avec succ√®s!\n"
                f"üìä Total trait√©: {self.processed_count}\n"
                f"‚ú® POIs PREMIUM cr√©√©s: {self.success_count}\n"
                f"‚è≠Ô∏è Ignor√©s: {self.skip_count}\n"
                f"‚ùå Erreurs: {self.error_count}\n"
                f"‚è±Ô∏è Dur√©e totale: {total_time/60:.1f} minutes\n"
                f"üí∞ Co√ªt total estim√©: ${self.total_cost_estimate:.2f}"
            )
            
            logger.info(f"üéâ {final_message}")
            self.log_to_database('SUCCESS', final_message, {
                'total_processed': self.processed_count,
                'success': self.success_count,
                'skipped': self.skip_count,
                'errors': self.error_count,
                'duration_seconds': total_time,
                'total_cost': self.total_cost_estimate,
                'cost_per_poi': self.total_cost_estimate / self.success_count if self.success_count > 0 else 0
            })
            
        except Exception as e:
            error_msg = f"Erreur fatale du crawler premium: {str(e)}"
            logger.error(f"üí• {error_msg}")
            logger.error(traceback.format_exc())
            self.log_to_database('ERROR', error_msg, {'traceback': traceback.format_exc()})
            raise


def main():
    """Point d'entr√©e principal du script premium"""
    try:
        print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          YORIMICHI PREMIUM CRAWLER - QUALITY EDITION         ‚ïë
‚ïë                                                              ‚ïë
‚ïë  üåü Mod√®les: GPT-4 Turbo pour tout                         ‚ïë
‚ïë  ‚ú® Descriptions: Uniques et captivantes                    ‚ïë
‚ïë  üìä Donn√©es: Extraction enrichie avec m√©tadonn√©es           ‚ïë
‚ïë  üéØ Qualit√©: Maximum, sans compromis                        ‚ïë
‚ïë                                                              ‚ïë
‚ïë  üí∞ Co√ªt estim√©: ~0.10$ par POI                            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """)
        
        crawler = YorimichiPremiumCrawler()
        crawler.run()
        
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è Arr√™t manuel du crawler premium")
        sys.exit(0)
    except Exception as e:
        logger.error(f"üí• Erreur fatale: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()